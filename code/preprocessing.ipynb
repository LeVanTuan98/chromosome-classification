{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "universal-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import yaml\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from fastprogress import progress_bar\n",
    "# import SimpleITK as sitk\n",
    "# import time\n",
    "# import nibabel as nib\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import torch \n",
    "# from torchvision import transforms as T\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch.nn.functional as F\n",
    "# import monai.transforms\n",
    "\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "# from utils.datasets import ImageDataset\n",
    "# from models.classifier import PLImageClassifier\n",
    "\n",
    "\n",
    "# class Inference:\n",
    "#     def __init__(self, cfg, trained_model_path, input_shape=(224, 224), scale_intensity=[0, 1], device='cpu'):\n",
    "#         self.cfg = cfg\n",
    "#         self.trained_model_path = trained_model_path\n",
    "#         self.device = device\n",
    "#         self.input_shape = input_shape\n",
    "#         self.labels_dict = [str(i) for i in range(cfg['model_params']['num_labels'])]\n",
    "\n",
    "#         # Set the augmentation\n",
    "# #         self.transforms = monai.transforms.Compose(\n",
    "# #             [\n",
    "# #                 monai.transforms.Resize(input_shape, size_mode='all'),\n",
    "# #                 monai.transforms.ToTensor(dtype=torch.float32)\n",
    "# #             ]\n",
    "# #         )\n",
    "#         self.transforms = T.Compose([\n",
    "#             T.ToPILImage(),\n",
    "#             T.Resize(self.input_shape),\n",
    "#             T.ToTensor()\n",
    "#         ])\n",
    "#         self.model = self.load_model()\n",
    "\n",
    "\n",
    "#     def load_model(self):\n",
    "#         print('Load the trained model in: ', self.trained_model_path)  \n",
    "#         model = PLImageClassifier(self.cfg)\n",
    "#         weights = torch.load(self.trained_model_path, map_location='cpu')['state_dict']\n",
    "#         model.load_state_dict(weights, strict=False)\n",
    "#         model = model.to(self.device)\n",
    "#         model.eval()\n",
    "\n",
    "#         return model\n",
    "\n",
    "#     def predict(self, img):\n",
    "#         x = self.transforms(img).unsqueeze(0).to(self.device)\n",
    "# #         print(x.shape)\n",
    "#         y = self.model(x)\n",
    "# #         print(y.shape)\n",
    "#         y = torch.argmax(y, dim=1).item()\n",
    "        \n",
    "#         return y\n",
    "\n",
    "# def read_config(cfg_path):\n",
    "#     with open(cfg_path) as file:\n",
    "#         cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "#     return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rational-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_ckpt = 'chromosome_classification-200-20220511-050334'\n",
    "    \n",
    "# cfg_path = glob.glob(f'./experiments/{dir_ckpt}/base*.yml')[-1]\n",
    "# ckpt_path = f'./experiments/{dir_ckpt}/best_model.pth'\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# cfg = read_config(cfg_path)\n",
    "# cfg['model_params']['pretrained'] = False\n",
    "# t_params = cfg['train_params']\n",
    "# # t_params['n_slices'] = cfg['model_params']['in_channels']\n",
    "\n",
    "# label_list = [str(i) for i in range(cfg['model_params']['num_labels'])]\n",
    "\n",
    "# model = Inference(cfg, ckpt_path, input_shape=t_params['input_shape'], device=device)\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "checked-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/121 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_folder = 'train_data'\n",
    "path = './Pussandata/anh phan tich/huuson.tiff/binh thuong'\n",
    "img_dir = glob.glob(path +'/*.tiff')\n",
    "\n",
    "# img_dir = glob.glob('./data/test/16/*.jpg')\n",
    "\n",
    "\n",
    "for img_file in tqdm(img_dir):\n",
    "    # Step 1: Read image\n",
    "    img = cv2.imread(img_file)\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(5, 15))\n",
    "    ax[0].imshow(img)\n",
    "    print(img.shape)\n",
    "\n",
    "    # Step 2: Threshold and find coutour\n",
    "    h_img, w_img, _ = img.shape\n",
    "\n",
    "    # Threshold\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # print(gray_img.shape)\n",
    "    # ax.imshow(gray_img, cmap='gray')\n",
    "    _, pre_thresh = cv2.threshold(gray_img, 254, 255, cv2.THRESH_BINARY)\n",
    "    ax[1].imshow(pre_thresh, cmap='gray')\n",
    "    thresh_img = cv2.morphologyEx(pre_thresh, cv2.MORPH_CLOSE, np.ones(shape=(3, 3), dtype=np.uint8))\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) # Use contour to extract sigle chromosome\n",
    "    cv2.drawContours(img, contours, -1, (255, 0, 0), 2)\n",
    "    ax[2].imshow(img)\n",
    "\n",
    "    # Step3: Bounding box\n",
    "    boxes = [] # Boxes include box for each chromosome, box = [x_center, y_center, w, h]\n",
    "    for i, contour in enumerate(contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        if 10< w < 200 and 10< h < 200 and x > 10 and y > 10: # remove outliers\n",
    "#             cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
    "\n",
    "            sub_img = img[y:y+h, x:x+w]\n",
    "            sub_img = Blackground(sub_img, size=256) # add black background for single chromosome\n",
    "\n",
    "            sub_img = torch.from_numpy(sub_img)\n",
    "    #         print(sub_img.shape)\n",
    "            label = model.predict(sub_img)\n",
    "            x = (x + w / 2) / w_img\n",
    "            y = (y + h / 2) / h_img # normalize bbox\n",
    "            w = w / w_img\n",
    "            h = h / h_img\n",
    "            boxes.append([label, x, y, w, h])\n",
    "    txt_dir = path.replace('anh phan tich', 'bouding box')\n",
    "    txt_file = txt_dir +'/'+ img_file.split('/')[-1].replace('tiff', 'txt')\n",
    "    if not os.path.exists(txt_dir):\n",
    "        os.makedirs(txt_dir, exist_ok=True)\n",
    "        \n",
    "    with open(f'{txt_file}', 'w') as file:\n",
    "        for box in boxes:\n",
    "            file.write(' '.join([str(s) for s in box]) + '\\n')\n",
    "            \n",
    "print('Done')\n",
    "# ax.imshow(img)\n",
    "#remove ảnh gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "another-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Blackground(img, size):  # add single chromosome onto black background\n",
    "    h, w = img.shape[:2]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    roi = 255 - cv2.inRange(img, 1, 250)  # seperate single chromosome\n",
    "    # cv2.imshow('img', img)\n",
    "    # cv2.imshow('roi',roi)\n",
    "    roi = img - roi\n",
    "    h, w = roi.shape\n",
    "\n",
    "    result = np.zeros(shape=(3, size, size), dtype=np.uint8)  # make background with shape =(size,size)\n",
    "    i = int((size - h) / 2)\n",
    "    j = int((size - w) / 2)\n",
    "    result[:, i:i + h, j:j + w] = [roi, roi, roi]  # add single chromosome in the center of the above background\n",
    "    # cv2.imshow('result',result)\n",
    "    # cv2.waitKey()\n",
    "    return result.transpose(1, 2, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
